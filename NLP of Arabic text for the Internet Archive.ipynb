{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge Processing of ISIS Content\n",
    "This notebook contains information regarding the efforts of **Kamal Kamalaldin** and **Will Fitzgerald** in filtering ISIS contents that violate the terms of service from the Internet Archive (www.archive.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. **Python 3+** (current version is 3.5). This should be a straightforward thing to [install](https://www.python.org/downloads/)\n",
    "2. [**pip3**](https://pip.pypa.io/en/stable/installing/) or [**homebrew**](http://brew.sh/) command line tools (homeb. We will be using these tools in install other, more essential tools.\n",
    "3. **The Internet Archive command tools**, refered to throughout this notebook as *ia*. Here we use pip to install *ia*. It can also be installed using any of the previous command line tools one or more of which we just installed. Because of some issues with using imported modules while using jupyter (IPython notebook), **if you plan on using this notebook to run all the code**, we need to specify that we want to install the *ia* tools in the directory in which jupyter can find the module. Follow the pip3 execurable command before continuing. \n",
    "    - using homebrew: \"brew install internetarchive\"\n",
    "    - using pip3: \"pip3 install internetarchive\"\n",
    "4. [** Vowpal Wabbit**](http://hunch.net/~vw/). For macs, this can be installed using homebrew. For windows, perhaps [scoop](http://scoop.sh/) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! EDIT THE FILEPATH AND DELETE THIS LINE BEFORE EXECUTING\n",
    "! DONT EXECUTE UNTIL EDITING PATH IN CAPS pip3 install internetarchive -t /PATH/TO/CURRENT/ANACONDA/ENVIRONMENT/lib/python3.5/site-packages/internetarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import internetarchive as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "At the beginning of our endeavour, we attempted to categories arabic item entries into those violating the term of service, and those who do not. Items can violate the term of service if they are \"deemed offensive, disturbing, pornographic, racist, sexist, bizarre, misleading, fraudulent, or otherwise objectionable\" (Terms of Service, Internet Archive). As Kamal was the only person knowledgable enough in the Arabic language to understand and categorize items, it was quickly aparent that the rate at which items were categorized was too slow to attain the sufficient data to train a model. Therefore, another approach was considered.\n",
    "\n",
    "For the data portion, a director tree was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of ~5,000 items that included ISIS/ISIL or their arabic counterparts in them was collected. This collection would represent items that were ISIS related, and assumed to be malicious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, items with \"الدولة الاسلامية\", the arabic equivalent of Islamic State are searched for. We first examine the number of items returned, then we download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data/ISIS\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"الدولة الاسلامية\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"الدولة الاسلامية\" -i >> data/ISIS/existingIDs.txt.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l data/ISIS/existingIDs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter an error running these commands, ensure that you have the latest version of *ia* installed (See **Requirements** above). \n",
    "\n",
    "The -i tag specifies that we are only interested in the ID of the item, not the whole file (we will download those later). The -n tag specifies that we only want to know the number of search results for now.\n",
    "In the third line we ask for the ID to be downloaded and put in a file called ISIS.txt in the data directory. The head command allows us to examine the first three lines of the file we jsut created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the IDs from the file and download the metadata for each item. The files are downloaded for future inspection and cashing. **This is currently a time consuming procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data/ISIS/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getIDsFromFile(file):\n",
    "    IDs = []\n",
    "    print(\"Openning ID file...\")\n",
    "    print(\"Reading IDs from file...\")\n",
    "    for line in ISIS_ID_file:\n",
    "        IDs.append(line.rstrip())\n",
    "    print(\"Finished reading IDs from file. Closing file...\")\n",
    "    ISIS_ID_file.close()\n",
    "    print(\"ID File closed!\")\n",
    "    return IDs\n",
    "\n",
    "def downloadMetaFiles(IDs, directoryToSave):\n",
    "    \n",
    "    print(\"Downloading \" + str(len(IDs)) + \" item metadatas...\")\n",
    "    count = 1\n",
    "    for ID in IDs:\n",
    "        file = open(directoryToSave + ID + \".txt\", 'w')\n",
    "        meta = ia.get_item(ID).metadata\n",
    "        json.dump(meta, file)\n",
    "        if(count % 31 == 0):\n",
    "            print('downloaded ' + str(count) + ' metadata files so far!')\n",
    "        count += 1\n",
    "    print(\"Successefully downloaded \" + str(len(IDs)) + \" metadatas!\")\n",
    "\n",
    "\n",
    "ISIS_ID_file = open('data/ISIS/IDList.txt', 'r')\n",
    "posIDs = getIDsFromFile(ISIS_ID_file)\n",
    "downloadMetaFiles(posIDs, \"data/ISIS/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the metadata readily available in files, and I/O should take a much faster time to retreive the data rather than downloading them again if need be. We define the methods below to extract the metadata back from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def getMetaFromFile(fileDir):\n",
    "    file = open(fileDir, 'r')\n",
    "    try:\n",
    "        jObject = json.load(file)\n",
    "        return jObject\n",
    "    except ValueError:\n",
    "        print(\"Error reading JSON from \" + fileDir)\n",
    "    \n",
    "def readMetaTextInDirectory(directory):\n",
    "    return [getMetaFromFile(directory + os.sep + f) \n",
    "            for f in os.listdir(directory)\n",
    "            if  f.endswith('.txt')]\n",
    "\n",
    "posMetadata = readMetaTextInDirectory('data/ISIS/metadata')\n",
    "len(posMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of metadata will be the actual data points which we will process, develop our model with, and test against!\n",
    "We need to get it in VW format first. To see what that looks like, skim through the [input format page](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format). In a nutshell, each data point (metadata entry) must be on its own line. Each line must begin with a +1 or -1 to signify if it's a positive or negative example (ISIS or not ISIS, respectively, in this case), followed by a pipe \"|\" that separated each namespace and its features. The format for namespace and features is: \"namespac1e: feature1 |namespace2: feature2 |namespeace3: feature3...\\n\". \"|\" and \":\" are reserved for VW, so we must distinguish their occurance in the metadata from their occrance in the VW format. We replace \"|\" with \"PIPE\" and \":\" with \"COLON\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proccessText(text):\n",
    "    newText = text.replace(':', 'COLON').replace('|', 'PIPE').replace('\\n', ' ').replace('\\r', ' ').replace(\"@\", ' ')\n",
    "    newText = newText.replace('<br>', ' ').replace(\"<\\br>\", ' ')\n",
    "    newText = newText.replace('الدولة الاسلامية', ' ')\n",
    "    newText = newText.replace('سكر', ' ')\n",
    "    n2 = []\n",
    "    for word in newText.split():\n",
    "        if len(word) < 50:\n",
    "            n2.append(word)\n",
    "    return ' '.join(n2[:500])\n",
    "\n",
    "def metadataToVWline(metadata: dict, positive: bool):\n",
    "    ignored_keys = ['mediatype', 'sound', 'color', 'curation']\n",
    "    data = ''\n",
    "    if metadata is None:\n",
    "        print('Found null metadata. Skipping.')\n",
    "    else:\n",
    "        for key in metadata:\n",
    "            if key in ignored_keys:\n",
    "                continue\n",
    "            else:\n",
    "                if(type(metadata[key]) == list):\n",
    "                    string = ' '.join(metadata[key])\n",
    "                    data += key + ' ' + proccessText(string) + \" |\"\n",
    "                else:\n",
    "                    data += key + ' ' + proccessText(metadata[key]) + \" |\"\n",
    "        #remove last trailing pipe and add new line\n",
    "        data = data.rstrip('|')\n",
    "        data += '\\n'\n",
    "\n",
    "        if(positive):\n",
    "            data = \"+1 |\" + data\n",
    "        else:\n",
    "            data = \"-1 |\" + data\n",
    "        \n",
    "    return data\n",
    "\n",
    "#     ID = metadata.get('identifier', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     uploader = metadata.get('uploader', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     title = metadata.get('title', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     subject = metadata.get('subject', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     description = metadata.get('description', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     publishDate = metadata.get('publishdate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     addedDate = metadata.get('addeddate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     collection = metadata.get('collection', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     ID = metadata['identifier']\n",
    "#     uploader = metadata['uploader']\n",
    "#     title = metadata['title']\n",
    "#     subject = metadata['subject']\n",
    "#     description = metadata['description']\n",
    "#     publishDate = metadata['publicdate']\n",
    "#     addedDate = metadata['addeddate']\n",
    "#     collection = metadata['collection']\n",
    "#     print(metadata)\n",
    "    \n",
    "#     data = \"identifier: \".join(ID) + \" | uploader: \".join(uploader) + \" | title: \".join(title) + \" | subject: \"\n",
    "#     data = data + ''.join(subject) + \" | description\".join(description) + \" | publishDate: \".join(publishDate)\n",
    "#     data = data + \" | addedDate: \".join(addedDate) + \" | collection: \".join(collection) + \"\\n\"\n",
    "\n",
    "\n",
    "def writeVWlinesToFile(vwLines, file):\n",
    "    print(\"opening Vwneg.txt file to write metadata\")\n",
    "    print(\"Writing lines to file...\")\n",
    "    for line in vwLines:\n",
    "        file.write(line)\n",
    "    print(\"Closing file...\")\n",
    "    file.close()\n",
    "    print(\"File closed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posVwLines = []\n",
    "for meta in posMetadata:\n",
    "    posVwLines.append(metadataToVWline(meta, True))\n",
    "    \n",
    "ISIS_Metadata_vw_file = open('data/ISIS/VWpos.txt', 'w')\n",
    "writeVWlinesToFile(posVwLines, ISIS_Metadata_vw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* ISIS content that violated terms of service). The accurasy of our assumption was recorded as \n",
    "\n",
    "\n",
    "$AccViol = \\frac{v(s)}{l(s)}$\n",
    "\n",
    "where $v(s)$ is the number of items violating the terms of service in the sample $s$ and $l(s)$ is the number of items in $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.shuffle(metadata)\n",
    "# AccViol = 27/30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main points:\n",
    "- get negative data\n",
    "- run it all through VW\n",
    "- get a proccess to get new data (IDs + metadata)\n",
    "\n",
    "##### Arabic-non-ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another collectino of ~5,000 items that included general arabic words was collected. This collection would represent items that included arabic text but were not violating of the terms of service. A general keyword was needed for searching content not related to ISIS. سكر (sugar) was chosen. This keyword surprisingly collected a lot of quran recitations, which is good since that means the data will force the model to not discremenate based on religious text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data/Arabic-non-ISIS\n",
    "! mkdir data/Arabic-non-ISIS/metadata\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"سكر\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"سكر\" -i > data/Arabic-non-ISIS/existingIDs.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l data/Arabic-non-ISIS/metadata/existingIDs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SINCE THERE COULD BE A LOT OF METADATA TO DOWNLOAD, PLEASE FEEL FREE TO STOP THIS KERNEL PROCCESS MANUALLY WHEN YOU THINK YOU HAVE ENOUGH DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Arabic_non_ISIS_ID_file = open('data/Arabic-non-ISIS/existingIDs.txt', 'r')\n",
    "negIDs = getIDsFromFile(Arabic_non_ISIS_ID_file)\n",
    "downloadMetaFiles(posIDs, \"data/Arabic-non-ISIS/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the metadata that we downloaded into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negMetadata = readMetaTextInDirectory('data/Arabic-non-ISIS/metadata')\n",
    "len(negMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the items in this list into lines readable by vw, and then we write these lines to a file for backup and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negVwLines = []\n",
    "for meta in negMetadata:\n",
    "    negVwLines.append(metadataToVWline(meta, False))\n",
    "    \n",
    "Arabic_metadata_VW_file = open('data/Arabic-non-ISIS/VWneg.txt', 'w')\n",
    "writeVWlinesToFile(negVwLines, Arabic_metadata_VW_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* general arabic content that do not violate the terms of service). The accuracy of our assumption was recorded as $AccAra$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of negative and positive examples are combined and shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allVWLines = posVwLines + negVwLines\n",
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(allVWLines)\n",
    "len(allVWLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split in half the shuffled vw lines into training and testing data. We write both to their appropriate file. *sentiment.tr* will be our training data, and *sentiment.te* will be our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            h.write(ex)\n",
    "writeToVWFile('data/sentiment.tr', allVWLines[:1400])\n",
    "writeToVWFile('data/sentiment.te', allVWLines[1400:])\n",
    "!wc -l data/sentiment.tr data/sentiment.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the model against our training data and cross our fingers that it learns something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!vw --binary data/sentiment.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see what it looks like, it is time to train vw over the data sufficiently (with many passes) and produce a model file that will be used to make predictions against the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model in data/sentiment.model ! Time to run it against testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! vw --binary -t -i data/sentiment.model -p data/sentiment.te.pred data/sentiment.te --audit > data/audit_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the model with an --audit option to output the verbose working of VW. This output is redirected to a file called *audit_log.txt* in the data directory. This file can be useful for debugging and getting insight into our data and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Placeholder code\")\n",
    "ia search 'الصداقة' -i -n\n",
    "ia search 'سكر' -i -n\n",
    "\n",
    "AccAra = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
