{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge Processing of ISIS Content\n",
    "This notebook contains information regarding the efforts of **Kamal Kamalaldin** and **Will Fitzgerald** in filtering ISIS contents that violate the terms of service from the Internet Archive (www.archive.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. **Python 3+** (current version is 3.5). This should be a straightforward thing to [install](https://www.python.org/downloads/)\n",
    "2. [**pip3**](https://pip.pypa.io/en/stable/installing/) or [**homebrew**](http://brew.sh/) command line tools (homeb. We will be using these tools in install other, more essential tools.\n",
    "3. **The Internet Archive command tools**, refered to throughout this notebook as *ia*. Here we use pip to install *ia*. It can also be installed using any of the previous command line tools one or more of which we just installed. Because of some issues with using imported modules while using jupyter (IPython notebook), **if you plan on using this notebook to run all the code**, we need to specify that we want to install the *ia* tools in the directory in which jupyter can find the module. Follow the pip3 execurable command before continuing. \n",
    "    - using homebrew: \"brew install internetarchive\"\n",
    "    - using pip3: \"pip3 install internetarchive\"\n",
    "4. [** Vowpal Wabbit**](http://hunch.net/~vw/). For macs, this can be installed using homebrew. For windows, perhaps [scoop](http://scoop.sh/) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! EDIT THE FILEPATH AND DELETE THIS LINE BEFORE EXECUTING\n",
    "! DONT EXECUTE UNTIL EDITING PATH IN CAPS pip3 install internetarchive -t /PATH/TO/CURRENT/ANACONDA/ENVIRONMENT/lib/python3.5/site-packages/internetarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import internetarchive as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "At the beginning of our endeavour, we attempted to categories arabic item entries into those violating the term of service, and those who do not. Items can violate the term of service if they are \"deemed offensive, disturbing, pornographic, racist, sexist, bizarre, misleading, fraudulent, or otherwise objectionable\" (Terms of Service, Internet Archive). As Kamal was the only person knowledgable enough in the Arabic language to understand and categorize items, it was quickly aparent that the rate at which items were categorized was too slow to attain the sufficient data to train a model. Therefore, another approach was considered.\n",
    "\n",
    "For the data portion, a director tree was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of ~5,000 items that included ISIS/ISIL or their arabic counterparts in them was collected. This collection would represent items that were ISIS related, and assumed to be malicious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, items with \"الدولة الاسلامية\", the arabic equivalent of Islamic State are searched for. We first examine the number of items returned, then we download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data/ISIS\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"الدولة الاسلامية\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"الدولة الاسلامية\" -i >> data/ISIS/existingIDs.txt.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l data/ISIS/existingIDs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter an error running these commands, ensure that you have the latest version of *ia* installed (See **Requirements** above). \n",
    "\n",
    "The -i tag specifies that we are only interested in the ID of the item, not the whole file (we will download those later). The -n tag specifies that we only want to know the number of search results for now.\n",
    "In the third line we ask for the ID to be downloaded and put in a file called ISIS.txt in the data directory. The head command allows us to examine the first three lines of the file we jsut created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the IDs from the file and download the metadata for each item. The files are downloaded for future inspection and cashing. **This is currently a time consuming procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data/ISIS/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getIDsFromFile(file):\n",
    "    IDs = []\n",
    "    print(\"Openning ID file...\")\n",
    "    print(\"Reading IDs from file...\")\n",
    "    for line in ISIS_ID_file:\n",
    "        IDs.append(line.rstrip())\n",
    "    print(\"Finished reading IDs from file. Closing file...\")\n",
    "    ISIS_ID_file.close()\n",
    "    print(\"ID File closed!\")\n",
    "    return IDs\n",
    "\n",
    "def downloadMetaFiles(IDs, directoryToSave):\n",
    "    \n",
    "    print(\"Downloading \" + str(len(IDs)) + \" item metadatas...\")\n",
    "    count = 1\n",
    "    for ID in IDs:\n",
    "        file = open(directoryToSave + ID + \".txt\", 'w')\n",
    "        meta = ia.get_item(ID).metadata\n",
    "        json.dump(meta, file)\n",
    "        if(count % 31 == 0):\n",
    "            print('downloaded ' + str(count) + ' metadata files so far!')\n",
    "        count += 1\n",
    "    print(\"Successefully downloaded \" + str(len(IDs)) + \" metadatas!\")\n",
    "\n",
    "\n",
    "ISIS_ID_file = open('data/ISIS/IDList.txt', 'r')\n",
    "posIDs = getIDsFromFile(ISIS_ID_file)\n",
    "downloadMetaFiles(posIDs, \"data/ISIS/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the metadata readily available in files, and I/O should take a much faster time to retreive the data rather than downloading them again if need be. We define the methods below to extract the metadata back from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def getMetaFromFile(fileDir):\n",
    "    file = open(fileDir, 'r')\n",
    "    try:\n",
    "        jObject = json.load(file)\n",
    "        return jObject\n",
    "    except ValueError:\n",
    "        print(\"Error reading JSON from \" + fileDir)\n",
    "    \n",
    "def readMetaTextInDirectory(directory):\n",
    "    return [getMetaFromFile(directory + os.sep + f) \n",
    "            for f in os.listdir(directory)\n",
    "            if  f.endswith('.txt')]\n",
    "\n",
    "posMetadata = readMetaTextInDirectory('data/ISIS/metadata')\n",
    "len(posMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of metadata will be the actual data points which we will process, develop our model with, and test against!\n",
    "We need to get it in VW format first. To see what that looks like, skim through the [input format page](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format). In a nutshell, each data point (metadata entry) must be on its own line. Each line must begin with a +1 or -1 to signify if it's a positive or negative example (ISIS or not ISIS, respectively, in this case), followed by a pipe \"|\" that separated each namespace and its features. The format for namespace and features is: \"namespac1e: feature1 |namespace2: feature2 |namespeace3: feature3...\\n\". \"|\" and \":\" are reserved for VW, so we must distinguish their occurance in the metadata from their occrance in the VW format. We replace \"|\" with \"PIPE\" and \":\" with \"COLON\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proccessText(text):\n",
    "    newText = text.replace(':', 'COLON').replace('|', 'PIPE').replace('\\n', ' ').replace('\\r', ' ').replace(\"@\", ' ')\n",
    "    newText = newText.replace('<br>', ' ').replace(\"<\\br>\", ' ')\n",
    "    newText = newText.replace('الدولة الاسلامية', ' ')\n",
    "    newText = newText.replace('سكر', ' ')\n",
    "    n2 = []\n",
    "    for word in newText.split():\n",
    "        if len(word) < 50:\n",
    "            n2.append(word)\n",
    "    return ' '.join(n2[:500])\n",
    "\n",
    "def metadataToVWline(metadata: dict, positive: bool):\n",
    "    ignored_keys = ['mediatype', 'sound', 'color', 'curation']\n",
    "    data = ''\n",
    "    if metadata is None:\n",
    "        print('Found null metadata. Skipping.')\n",
    "    else:\n",
    "        for key in metadata:\n",
    "            if key in ignored_keys:\n",
    "                continue\n",
    "            else:\n",
    "                if(type(metadata[key]) == list):\n",
    "                    string = ' '.join(metadata[key])\n",
    "                    data += key + ' ' + proccessText(string) + \" |\"\n",
    "                else:\n",
    "                    data += key + ' ' + proccessText(metadata[key]) + \" |\"\n",
    "        #remove last trailing pipe and add new line\n",
    "        data = data.rstrip('|')\n",
    "        data += '\\n'\n",
    "\n",
    "        if(positive):\n",
    "            data = \"+1 |\" + data\n",
    "        else:\n",
    "            data = \"-1 |\" + data\n",
    "        \n",
    "    return data\n",
    "\n",
    "#     ID = metadata.get('identifier', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     uploader = metadata.get('uploader', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     title = metadata.get('title', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     subject = metadata.get('subject', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     description = metadata.get('description', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     publishDate = metadata.get('publishdate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     addedDate = metadata.get('addeddate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     collection = metadata.get('collection', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     ID = metadata['identifier']\n",
    "#     uploader = metadata['uploader']\n",
    "#     title = metadata['title']\n",
    "#     subject = metadata['subject']\n",
    "#     description = metadata['description']\n",
    "#     publishDate = metadata['publicdate']\n",
    "#     addedDate = metadata['addeddate']\n",
    "#     collection = metadata['collection']\n",
    "#     print(metadata)\n",
    "    \n",
    "#     data = \"identifier: \".join(ID) + \" | uploader: \".join(uploader) + \" | title: \".join(title) + \" | subject: \"\n",
    "#     data = data + ''.join(subject) + \" | description\".join(description) + \" | publishDate: \".join(publishDate)\n",
    "#     data = data + \" | addedDate: \".join(addedDate) + \" | collection: \".join(collection) + \"\\n\"\n",
    "\n",
    "\n",
    "def writeVWlinesToFile(vwLines, file):\n",
    "    print(\"opening Vwneg.txt file to write metadata\")\n",
    "    print(\"Writing lines to file...\")\n",
    "    for line in vwLines:\n",
    "        file.write(line)\n",
    "    print(\"Closing file...\")\n",
    "    file.close()\n",
    "    print(\"File closed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening Vwneg.txt file to write metadata\n",
      "Writing lines to file...\n",
      "Closing file...\n",
      "File closed!\n"
     ]
    }
   ],
   "source": [
    "posVwLines = []\n",
    "for meta in posMetadata:\n",
    "    posVwLines.append(metadataToVWline(meta, True))\n",
    "    \n",
    "ISIS_Metadata_vw_file = open('data/ISIS/VWpos.txt', 'w')\n",
    "writeVWlinesToFile(posVwLines, ISIS_Metadata_vw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* ISIS content that violated terms of service). The accurasy of our assumption was recorded as \n",
    "\n",
    "\n",
    "$AccViol = \\frac{v(s)}{l(s)}$\n",
    "\n",
    "where $v(s)$ is the number of items violating the terms of service in the sample $s$ and $l(s)$ is the number of items in $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.shuffle(metadata)\n",
    "# AccViol = 27/30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main points:\n",
    "- get negative data\n",
    "- run it all through VW\n",
    "- get a proccess to get new data (IDs + metadata)\n",
    "\n",
    "##### Arabic-non-ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another collectino of ~5,000 items that included general arabic words was collected. This collection would represent items that included arabic text but were not violating of the terms of service. A general keyword was needed for searching content not related to ISIS. سكر (sugar) was chosen. This keyword surprisingly collected a lot of quran recitations, which is good since that means the data will force the model to not discremenate based on religious text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir data/Arabic-non-ISIS\n",
    "! mkdir data/Arabic-non-ISIS/metadata\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"سكر\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"سكر\" -i > data/Arabic-non-ISIS/existingIDs.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l data/Arabic-non-ISIS/metadata/existingIDs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SINCE THERE COULD BE A LOT OF METADATA TO DOWNLOAD, PLEASE FEEL FREE TO STOP THIS KERNEL PROCCESS MANUALLY WHEN YOU THINK YOU HAVE ENOUGH DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Arabic_non_ISIS_ID_file = open('data/Arabic-non-ISIS/existingIDs.txt', 'r')\n",
    "negIDs = getIDsFromFile(Arabic_non_ISIS_ID_file)\n",
    "downloadMetaFiles(posIDs, \"data/Arabic-non-ISIS/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the metadata that we downloaded into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading JSON from data/Arabic-non-ISIS/metadata/MA2436724357324747374A12.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1826"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negMetadata = readMetaTextInDirectory('data/Arabic-non-ISIS/metadata')\n",
    "len(negMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the items in this list into lines readable by vw, and then we write these lines to a file for backup and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found null metadata. Skipping.\n"
     ]
    }
   ],
   "source": [
    "negVwLines = []\n",
    "for meta in negMetadata:\n",
    "    negVwLines.append(metadataToVWline(meta, False))\n",
    "    \n",
    "# Arabic_metadata_VW_file = open('data/Arabic-non-ISIS/VWneg.txt', 'w')\n",
    "# writeVWlinesToFile(negVwLines, Arabic_metadata_VW_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* general arabic content that do not violate the terms of service). The accuracy of our assumption was recorded as $AccAra$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of negative and positive examples are combined and shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2889"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allVWLines = posVwLines + negVwLines\n",
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(allVWLines)\n",
    "len(allVWLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split in half the shuffled vw lines into training and testing data. We write both to their appropriate file. *sentiment.tr* will be our training data, and *sentiment.te* will be our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1400 data/sentiment.tr\n",
      "    1488 data/sentiment.te\n",
      "    2888 total\n"
     ]
    }
   ],
   "source": [
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            h.write(ex)\n",
    "writeToVWFile('data/sentiment.tr', allVWLines[:1400])\n",
    "writeToVWFile('data/sentiment.te', allVWLines[1400:])\n",
    "!wc -l data/sentiment.tr data/sentiment.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the model against our training data and cross our fingers that it learns something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000      554\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000      172\n",
      "0.500000 0.500000            4            4.0   1.0000  -1.0000       24\n",
      "0.250000 0.000000            8            8.0  -1.0000  -1.0000      528\n",
      "0.125000 0.000000           16           16.0   1.0000   1.0000      100\n",
      "0.093750 0.062500           32           32.0  -1.0000  -1.0000      532\n",
      "0.062500 0.031250           64           64.0  -1.0000  -1.0000      519\n",
      "0.031250 0.000000          128          128.0   1.0000   1.0000       49\n",
      "0.019531 0.007812          256          256.0  -1.0000  -1.0000      533\n",
      "0.015625 0.011719          512          512.0  -1.0000  -1.0000      552\n",
      "0.020508 0.025391         1024         1024.0  -1.0000  -1.0000      527\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1400\n",
      "passes used = 1\n",
      "weighted example sum = 1400.000000\n",
      "weighted label sum = -334.000000\n",
      "average loss = 0.016429\n",
      "best constant = -0.238571\n",
      "best constant's loss = 0.943084\n",
      "total feature number = 507821\n"
     ]
    }
   ],
   "source": [
    "!vw --binary data/sentiment.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see what it looks like, it is time to train vw over the data sufficiently (with many passes) and produce a model file that will be used to make predictions against the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000      554\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000      172\n",
      "0.500000 0.500000            4            4.0   1.0000  -1.0000       24\n",
      "0.250000 0.000000            8            8.0  -1.0000  -1.0000      528\n",
      "0.125000 0.000000           16           16.0  -1.0000  -1.0000      526\n",
      "0.093750 0.062500           32           32.0  -1.0000  -1.0000      535\n",
      "0.062500 0.031250           64           64.0  -1.0000  -1.0000      524\n",
      "0.031250 0.000000          128          128.0   1.0000   1.0000       23\n",
      "0.023438 0.015625          256          256.0  -1.0000  -1.0000      525\n",
      "0.015625 0.007812          512          512.0  -1.0000  -1.0000      527\n",
      "0.022461 0.029297         1024         1024.0   1.0000   1.0000       22\n",
      "0.000000 0.000000         2048         2048.0   1.0000   1.0000       21 h\n",
      "0.000000 0.000000         4096         4096.0  -1.0000  -1.0000      527 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1260\n",
      "passes used = 4\n",
      "weighted example sum = 5040.000000\n",
      "weighted label sum = -1216.000000\n",
      "average loss = 0.000000 h\n",
      "best constant = -0.241270\n",
      "best constant's loss = 0.941789\n",
      "total feature number = 1827888\n"
     ]
    }
   ],
   "source": [
    "! vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a model in data/sentiment.model! Notice that the **average loss** is incredibly low when the model is finally created, which means that the extra passes improved the model, and the last time it ran through the model it could predict (persumably) all of the items correctly.\n",
    "Since we have a model, it is now time to run it against the testing data we left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = data/sentiment.te.pred\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.te\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000      526\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       29\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000      544\n",
      "0.000000 0.000000            8            8.0  -1.0000  -1.0000      531\n",
      "0.000000 0.000000           16           16.0  -1.0000  -1.0000      530\n",
      "0.031250 0.062500           32           32.0  -1.0000  -1.0000      528\n",
      "0.031250 0.031250           64           64.0  -1.0000  -1.0000      531\n",
      "0.015625 0.000000          128          128.0   1.0000   1.0000       30\n",
      "0.011719 0.007812          256          256.0  -1.0000  -1.0000      539\n",
      "0.015625 0.019531          512          512.0   1.0000   1.0000       28\n",
      "0.013672 0.011719         1024         1024.0  -1.0000  -1.0000      528\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1488\n",
      "passes used = 1\n",
      "weighted example sum = 1488.000000\n",
      "weighted label sum = -428.000000\n",
      "average loss = 0.012769\n",
      "best constant = -0.287634\n",
      "best constant's loss = 0.917266\n",
      "total feature number = 553291\n"
     ]
    }
   ],
   "source": [
    "! vw --binary -t -i data/sentiment.model -p data/sentiment.te.pred data/sentiment.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average loss is only 0.011, which means that the model is 98.9% accurate! Since this is a little suspecious, let us look at what the most predictive features were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!vw -i data/sentiment.model -t --invert_hash data/sentiment.model.readable data/sentiment.tr --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As copied from GettingTheMost vwnlp notebook: this says \"start from that pre-trained model; go into test mode (so that you don't adjust any of the weights of the model); store the resulting readable model (--invert_hash) into the specified file; and read from data/sentiment.tr (you have to re-read from the same training data).\n",
    "We can now look at data/sentiment.model.readable to see what's going on.\"\n",
    "\n",
    "The next step after making this readable copy of the model, we need to inspect it. To do that, we will sort the data by the feature weight (details in the refereed book). We look at the 30 most predictive features by -n30 at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection^opensource_movies:102092:0.130581\n",
      "description^باقية:220112:0.130282\n",
      "uploader^mail.ru:184078:0.110775\n",
      "collection^loggedin:13007:0.097585\n",
      "scanner^Archive:202608:0.088549\n",
      "scanner^HTML5:130839:0.088549\n",
      "scanner^Internet:229369:0.088549\n",
      "scanner^Uploader:37599:0.088549\n",
      "scanner^1.6.1:93547:0.086958\n",
      "uploader^oma_222:64648:0.084879\n",
      "title^في:64074:0.084830\n",
      "subject^العراق:205019:0.079868\n",
      "language^eng:144814:0.074802\n",
      "description^ولاية:255536:0.071500\n",
      "subject^Islam:246233:0.066713\n",
      "year^2014:139254:0.065090\n",
      "title^من:174608:0.064924\n",
      "uploader^gmail.com:184177:0.062123\n",
      "subject^shamikh1.info/vb;:43030:0.061989\n",
      "creator^ابو:151298:0.061721\n",
      "subject^ولاية:246401:0.061025\n",
      "uploader^fofo.bobo.82:169643:0.059059\n",
      "title^ولاية:23670:0.057314\n",
      "subject^في:202856:0.055348\n",
      "uploader^rock201110:34556:0.054932\n",
      "collection^iraq_middleeast:218420:0.053134\n",
      "collection^iraq_war:117857:0.053134\n",
      "collection^newsandpublicaffairs:164064:0.053134\n",
      "creator^المهاجر:132187:0.052726\n",
      "description^بغداد:143779:0.052462\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head -n30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Scanner\n",
    "Since we are not particularly intereted by what scanner was used to scan the items when they were added to the Interner Archive, we predict that the scanner namespace is not a necessarily important one, so we would like to remove it. Furthrmore, we don't want the model to be biased against items in one collection or another, so we remove that namespace as well. We remove these namespaces by ignoring them when we are proccessing the metadata. So we redefine that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metadataToVWline(metadata: dict, positive: bool):\n",
    "    ignored_keys = ['mediatype', 'sound', 'color', 'curation', 'collection', 'creator', 'scanner']\n",
    "    data = ''\n",
    "    if metadata is None:\n",
    "        print('Found null metadata. Skipping.')\n",
    "    else:\n",
    "        for key in metadata:\n",
    "            if key in ignored_keys:\n",
    "                continue\n",
    "            else:\n",
    "                if(type(metadata[key]) == list):\n",
    "                    string = ' '.join(metadata[key])\n",
    "                    data += key + ' ' + proccessText(string) + \" |\"\n",
    "                else:\n",
    "                    data += key + ' ' + proccessText(metadata[key]) + \" |\"\n",
    "        #remove last trailing pipe and add new line\n",
    "        data = data.rstrip('|')\n",
    "        data += '\\n'\n",
    "\n",
    "        if(positive):\n",
    "            data = \"+1 |\" + data\n",
    "        else:\n",
    "            data = \"-1 |\" + data\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-proccess the metadata and run it through vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found null metadata. Skipping.\n",
      "final_regressor = data/sentiment.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000      548\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000      168\n",
      "0.250000 0.000000            4            4.0   1.0000   1.0000       18\n",
      "0.125000 0.000000            8            8.0  -1.0000  -1.0000      522\n",
      "0.125000 0.125000           16           16.0  -1.0000  -1.0000      520\n",
      "0.093750 0.062500           32           32.0  -1.0000  -1.0000      529\n",
      "0.062500 0.031250           64           64.0  -1.0000  -1.0000      518\n",
      "0.031250 0.000000          128          128.0   1.0000   1.0000       17\n",
      "0.023438 0.015625          256          256.0  -1.0000  -1.0000      519\n",
      "0.013672 0.003906          512          512.0  -1.0000  -1.0000      521\n",
      "0.015625 0.017578         1024         1024.0   1.0000   1.0000       16\n",
      "0.000000 0.000000         2048         2048.0   1.0000   1.0000       15 h\n",
      "0.000000 0.000000         4096         4096.0  -1.0000  -1.0000      521 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1260\n",
      "passes used = 4\n",
      "weighted example sum = 5040.000000\n",
      "weighted label sum = -1216.000000\n",
      "average loss = 0.000000 h\n",
      "best constant = -0.241270\n",
      "best constant's loss = 0.941789\n",
      "total feature number = 1798008\n",
      "uploader^mail.ru:184078:0.232043\n",
      "description^باقية:220112:0.221594\n",
      "uploader^oma_222:64648:0.185178\n",
      "description^ولاية:255536:0.159717\n",
      "uploader^gmail.com:184177:0.145639\n",
      "subject^ولاية:246401:0.128112\n",
      "title^في:64074:0.125890\n",
      "uploader^fofo.bobo.82:169643:0.115585\n",
      "subject^العراق:205019:0.112157\n",
      "title^من:174608:0.107447\n",
      "language^ara:180529:0.098500\n",
      "title^ولاية:23670:0.092042\n",
      "geo_restricted^RU:96586:0.080350\n",
      "uploader^bk.ru:46778:0.078348\n",
      "language^eng:144814:0.077823\n",
      "description^<span:36207:0.077745\n",
      "uploader^enimey:49306:0.075644\n",
      "subject^Islam:246233:0.075184\n",
      "description^الخلافة:199369:0.073502\n",
      "description^</div><div><br:148532:0.071775\n",
      "title^الدولة:104376:0.071530\n",
      "title^نشيد:61049:0.071145\n",
      "description^بغداد:143779:0.070552\n",
      "publicdate^17COLON47COLON50:143779:0.070552\n",
      "description^الفلوجة:26953:0.069656\n",
      "addeddate^2015-04-13:241158:0.067901\n",
      "publicdate^2015-04-13:115895:0.067901\n",
      "description^جنود:66258:0.066845\n",
      "subject^في:202856:0.066719\n",
      "description^نصرة:106161:0.066290\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "posVwLines = []\n",
    "for meta in posMetadata:\n",
    "    posVwLines.append(metadataToVWline(meta, True))\n",
    "negVwLines = []\n",
    "for meta in negMetadata:\n",
    "    negVwLines.append(metadataToVWline(meta, False))\n",
    "allVWLines = posVwLines + negVwLines\n",
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(allVWLines)\n",
    "len(allVWLines)\n",
    "writeToVWFile('data/sentiment.tr', allVWLines[:1400])\n",
    "writeToVWFile('data/sentiment.te', allVWLines[1400:])\n",
    "! vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model\n",
    "!vw -i data/sentiment.model -t --invert_hash data/sentiment.model.readable data/sentiment.tr --quiet\n",
    "!cat data/sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head -n30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Placeholder code\")\n",
    "ia search 'الصداقة' -i -n\n",
    "ia search 'سكر' -i -n\n",
    "\n",
    "AccAra = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
