{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge Processing of ISIS Content\n",
    "This notebook contains information regarding the efforts of **Kamal Kamalaldin** and **Will Fitzgerald** in filtering ISIS contents that violate the terms of service from the Internet Archive (www.archive.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. **Python 3+** (current version is 3.5). This should be a straightforward thing to [install](https://www.python.org/downloads/)\n",
    "2. [**pip3**](https://pip.pypa.io/en/stable/installing/) or [**homebrew**](http://brew.sh/) command line tools (homeb. We will be using these tools in install other, more essential tools.\n",
    "3. **The Internet Archive command tools**, refered to throughout this notebook as *ia*. Here we use pip to install *ia*. It can also be installed using any of the previous command line tools one or more of which we just installed. Because of some issues with using imported modules while using jupyter (IPython notebook), **if you plan on using this notebook to run all the code**, we need to specify that we want to install the *ia* tools in the directory in which jupyter can find the module. Follow the pip3 execurable command before continuing. \n",
    "    - using homebrew: \"brew install internetarchive\"\n",
    "    - using pip3: \"pip3 install internetarchive\"\n",
    "4. [** Vowpal Wabbit**](http://hunch.net/~vw/). For macs, this can be installed using homebrew. For windows, perhaps [scoop](http://scoop.sh/) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.1\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: EDIT: command not found\n",
      "/bin/sh: DONT: command not found\n"
     ]
    }
   ],
   "source": [
    "! EDIT THE FILEPATH AND DELETE THIS LINE BEFORE EXECUTING\n",
    "! DONT EXECUTE UNTIL EDITING PATH IN CAPS pip3 install internetarchive -t /PATH/TO/CURRENT/ANACONDA/ENVIRONMENT/lib/python3.5/site-packages/internetarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import internetarchive as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "At the beginning of our endeavour, we attempted to categories arabic item entries into those violating the term of service, and those who do not. Items can violate the term of service if they are \"deemed offensive, disturbing, pornographic, racist, sexist, bizarre, misleading, fraudulent, or otherwise objectionable\" (Terms of Service, Internet Archive). As Kamal was the only person knowledgable enough in the Arabic language to understand and categorize items, it was quickly aparent that the rate at which items were categorized was too slow to attain the sufficient data to train a model. Therefore, another approach was considered.\n",
    "\n",
    "For the data portion, a director tree was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of ~5,000 items that included ISIS/ISIL or their arabic counterparts in them was collected. This collection would represent items that were ISIS related, and assumed to be malicious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, items with \"الدولة الاسلامية\", the arabic equivalent of Islamic State are searched for. We first examine the number of items returned, then we download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/ISIS: File exists\n",
      "Number of matching searches found : \n",
      "1051\n",
      "Donwloading item identifiers...\n",
      "Number of item identifiers downloaded:\n",
      "    1051 data/ISIS/IDList.txt\n"
     ]
    }
   ],
   "source": [
    "! mkdir data/ISIS\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"الدولة الاسلامية\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"الدولة الاسلامية\" -i > data/ISIS/IDList.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l data/ISIS/IDList.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter an error running these commands, ensure that you have the latest version of *ia* installed (See **Requirements** above). \n",
    "\n",
    "The -i tag specifies that we are only interested in the ID of the item, not the whole file (we will download those later). The -n tag specifies that we only want to know the number of search results for now.\n",
    "In the third line we ask for the ID to be downloaded and put in a file called ISIS.txt in the data directory. The head command allows us to examine the first three lines of the file we jsut created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the IDs from the file and download the metadata for each item with that identifier.\n",
    "**This is currently a time consuming procedure. Further improvements can be made by downloading the metadata files separately then manipulating them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openning ID file...\n",
      "Reading IDs from file...\n",
      "Finished reading IDs from file. Closing file...\n",
      "ID File closed!\n",
      "Downloading 1051 item metadatas...\n",
      "downloaded 31 metadata files so far!\n",
      "downloaded 62 metadata files so far!\n",
      "downloaded 93 metadata files so far!\n",
      "downloaded 124 metadata files so far!\n",
      "downloaded 155 metadata files so far!\n",
      "downloaded 186 metadata files so far!\n",
      "downloaded 217 metadata files so far!\n",
      "downloaded 248 metadata files so far!\n",
      "downloaded 279 metadata files so far!\n",
      "downloaded 310 metadata files so far!\n",
      "downloaded 341 metadata files so far!\n",
      "downloaded 372 metadata files so far!\n",
      "downloaded 403 metadata files so far!\n",
      "downloaded 434 metadata files so far!\n",
      "downloaded 465 metadata files so far!\n",
      "downloaded 496 metadata files so far!\n",
      "downloaded 527 metadata files so far!\n",
      "downloaded 558 metadata files so far!\n",
      "downloaded 589 metadata files so far!\n",
      "downloaded 620 metadata files so far!\n",
      "downloaded 651 metadata files so far!\n",
      "downloaded 682 metadata files so far!\n",
      "downloaded 713 metadata files so far!\n",
      "downloaded 744 metadata files so far!\n",
      "downloaded 775 metadata files so far!\n",
      "downloaded 806 metadata files so far!\n",
      "downloaded 837 metadata files so far!\n",
      "downloaded 868 metadata files so far!\n",
      "downloaded 899 metadata files so far!\n",
      "downloaded 930 metadata files so far!\n",
      "downloaded 961 metadata files so far!\n",
      "downloaded 992 metadata files so far!\n",
      "downloaded 1023 metadata files so far!\n",
      "Successefully downloaded 1051 metadatas!\n"
     ]
    }
   ],
   "source": [
    "def getIDsFromFile():\n",
    "    IDs = []\n",
    "    print(\"Openning ID file...\")\n",
    "    ISIS_ID_file = open('data/ISIS/IDList.txt', 'r')\n",
    "    print(\"Reading IDs from file...\")\n",
    "    for line in ISIS_ID_file:\n",
    "        IDs.append(line.rstrip())\n",
    "    print(\"Finished reading IDs from file. Closing file...\")\n",
    "    ISIS_ID_file.close()\n",
    "    print(\"ID File closed!\")\n",
    "    return IDs\n",
    "\n",
    "def getItemsMetadata(item_identifiers):\n",
    "    metadata_list = []\n",
    "    print(\"Downloading \" + str(len(item_identifiers)) + \" item metadatas...\")\n",
    "    count = 1\n",
    "    for ID in item_identifiers:\n",
    "        item = ia.get_item(ID)\n",
    "        metadata_list.append(item.metadata)\n",
    "        if(count % 31 ==0):\n",
    "            print('downloaded ' + str(count) + ' metadata files so far!')\n",
    "        count += 1\n",
    "    print(\"Successefully downloaded \" + str(len(metadata_list)) + \" metadatas!\")\n",
    "    return metadata_list\n",
    "\n",
    "IDs = getIDsFromFile()\n",
    "metadata_list = getItemsMetadata(IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of metadata will be the actual data points which we will process, develop our model with, and test against!\n",
    "We need to get it in VW format first. To see what that looks like, skim through the [input format page](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format). In a nutshell, each data point (metadata entry) must be on its own line. Each line must begin with a +1 or -1 to signify if it's a positive or negative example (ISIS or not ISIS, respectively, in this case), followed by a pipe \"|\" that separated each namespace and its features. The format for namespace and features is: \"namespac1e: feature1 |namespace2: feature2 |namespeace3: feature3...\\n\". \"|\" and \":\" are reserved for VW, so we must distinguish their occurance in the metadata from their occrance in the VW format. We replace \"|\" with \"PIPE\" and \":\" with \"COLON\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening Vwneg.txt file to write metadata\n",
      "Writing lines to Vwneg.txt ...\n",
      "Closing Vwneg.txt file...\n",
      "File Vwneg.txt closed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def metadataToVWline(metadata: dict, positive: bool):\n",
    "    ignored_keys = ['mediatype', 'sound', 'color', 'curation']\n",
    "    data = ''\n",
    "    for key in metadata:\n",
    "        if key in ignored_keys:\n",
    "            continue\n",
    "        else:\n",
    "            if(type(metadata[key]) == list):\n",
    "                data += key + ': ' + ' '.join(metadata[key]).replace(':', 'COLON').replace('|', 'PIPE') + \" |\"\n",
    "            else:\n",
    "                data += key + ': ' + metadata[key].replace(':', 'COLON').replace('|', 'PIPE') + \" |\"\n",
    "    #remove last trailing pipe and add new line\n",
    "    data = data.rstrip('|')\n",
    "    data += '\\n'\n",
    "    \n",
    "    if(positive):\n",
    "        data = \"+1 | \" + data\n",
    "    else:\n",
    "        data = \"-1 | \" + data\n",
    "        \n",
    "    return data\n",
    "#     ID = metadata.get('identifier', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     uploader = metadata.get('uploader', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     title = metadata.get('title', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     subject = metadata.get('subject', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     description = metadata.get('description', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     publishDate = metadata.get('publishdate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     addedDate = metadata.get('addeddate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     collection = metadata.get('collection', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     ID = metadata['identifier']\n",
    "#     uploader = metadata['uploader']\n",
    "#     title = metadata['title']\n",
    "#     subject = metadata['subject']\n",
    "#     description = metadata['description']\n",
    "#     publishDate = metadata['publicdate']\n",
    "#     addedDate = metadata['addeddate']\n",
    "#     collection = metadata['collection']\n",
    "#     print(metadata)\n",
    "    \n",
    "#     data = \"identifier: \".join(ID) + \" | uploader: \".join(uploader) + \" | title: \".join(title) + \" | subject: \"\n",
    "#     data = data + ''.join(subject) + \" | description\".join(description) + \" | publishDate: \".join(publishDate)\n",
    "#     data = data + \" | addedDate: \".join(addedDate) + \" | collection: \".join(collection) + \"\\n\"\n",
    "\n",
    "\n",
    "def writeVWlinesToFile(vwLines):\n",
    "    print(\"opening Vwneg.txt file to write metadata\")\n",
    "    ISIS_Metadata_vw_file = open('data/ISIS/VWneg.txt', 'w')\n",
    "    print(\"Writing lines to Vwneg.txt ...\")\n",
    "    for line in vwLines:\n",
    "        ISIS_Metadata_vw_file.write(line)\n",
    "    print(\"Closing Vwneg.txt file...\")\n",
    "    ISIS_Metadata_vw_file.close()\n",
    "    print(\"File Vwneg.txt closed!\")\n",
    "\n",
    "\n",
    "vwLines = []\n",
    "for meta in metadata_list:\n",
    "    vwLines.append(metadataToVWline(meta, True))\n",
    "writeVWlinesToFile(vwLines)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* ISIS content that violated terms of service). The accurasy of our assumption was recorded as $AccViol$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder code\n"
     ]
    }
   ],
   "source": [
    "print(\"Placeholder code\")\n",
    "\n",
    "AccViol = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arabic-non-ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another collectino of ~5,000 items that included general arabic words was collected. This collection would represent items that included arabic text but were not violating of the terms of service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder Code\n"
     ]
    }
   ],
   "source": [
    "print(\"Placeholder Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* general arabic content that do not violate the terms of service). The accuracy of our assumption was recorded as $AccAra$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder code\n"
     ]
    }
   ],
   "source": [
    "print(\"Placeholder code\")\n",
    "\n",
    "AccAra = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
