{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge Processing of ISIS Content\n",
    "This notebook contains information regarding the efforts of **Kamal Kamalaldin** and **Will Fitzgerald** in filtering ISIS contents that violate the terms of service from the Internet Archive (www.archive.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. **Python 3+** (current version is 3.5). This should be a straightforward thing to [install](https://www.python.org/downloads/)\n",
    "2. [**pip3**](https://pip.pypa.io/en/stable/installing/) or [**homebrew**](http://brew.sh/) command line tools (homeb. We will be using these tools in install other, more essential tools.\n",
    "3. **The Internet Archive command tools**, refered to throughout this notebook as *ia*. Here we use pip to install *ia*. It can also be installed using any of the previous command line tools one or more of which we just installed. Because of some issues with using imported modules while using jupyter (IPython notebook), **if you plan on using this notebook to run all the code**, we need to specify that we want to install the *ia* tools in the directory in which jupyter can find the module. Follow the pip3 execurable command before continuing. \n",
    "    - using homebrew: \"brew install internetarchive\"\n",
    "    - using pip3: \"pip3 install internetarchive\"\n",
    "4. [** Vowpal Wabbit**](http://hunch.net/~vw/). For macs, this can be installed using homebrew. For windows, perhaps [scoop](http://scoop.sh/) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! EDIT THE FILEPATH AND DELETE THIS LINE BEFORE EXECUTING\n",
    "! DONT EXECUTE UNTIL EDITING PATH IN CAPS pip3 install internetarchive -t /PATH/TO/CURRENT/ANACONDA/ENVIRONMENT/lib/python3.5/site-packages/internetarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import internetarchive as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "At the beginning of our endeavour, we attempted to categories arabic item entries into those violating the term of service, and those who do not. Items can violate the term of service if they are \"deemed offensive, disturbing, pornographic, racist, sexist, bizarre, misleading, fraudulent, or otherwise objectionable\" (Terms of Service, Internet Archive). As Kamal was the only person knowledgable enough in the Arabic language to understand and categorize items, it was quickly aparent that the rate at which items were categorized was too slow to attain the sufficient data to train a model. Therefore, another approach was considered.\n",
    "\n",
    "For the data portion, a director tree was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of ~5,000 items that included ISIS/ISIL or their arabic counterparts in them was collected. This collection would represent items that were ISIS related, and assumed to be malicious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, items with \"الدولة الاسلامية\", the arabic equivalent of Islamic State are searched for. We first examine the number of items returned, then we download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir ../data/ISIS\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"الدولة الاسلامية\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"الدولة الاسلامية\" -i >> data/ISIS/existingIDs.txt.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l ../data/ISIS/existingIDs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you encounter an error running these commands, ensure that you have the latest version of *ia* installed (See **Requirements** above). \n",
    "\n",
    "The -i tag specifies that we are only interested in the ID of the item, not the whole file (we will download those later). The -n tag specifies that we only want to know the number of search results for now.\n",
    "In the third line we ask for the ID to be downloaded and put in a file called ISIS.txt in the data directory. The head command allows us to examine the first three lines of the file we jsut created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the IDs from the file and download the metadata for each item. The files are downloaded for future inspection and cashing. **This is currently a time consuming procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir ../data/ISIS/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getIDsFromFile(file):\n",
    "    IDs = []\n",
    "    print(\"Openning ID file...\")\n",
    "    print(\"Reading IDs from file...\")\n",
    "    for line in ISIS_ID_file:\n",
    "        IDs.append(line.rstrip())\n",
    "    print(\"Finished reading IDs from file. Closing file...\")\n",
    "    ISIS_ID_file.close()\n",
    "    print(\"ID File closed!\")\n",
    "    return IDs\n",
    "\n",
    "def downloadMetaFiles(IDs, directoryToSave):\n",
    "    \n",
    "    print(\"Downloading \" + str(len(IDs)) + \" item metadatas...\")\n",
    "    count = 1\n",
    "    for ID in IDs:\n",
    "        file = open(directoryToSave + ID + \".txt\", 'w')\n",
    "        meta = ia.get_item(ID).metadata\n",
    "        json.dump(meta, file)\n",
    "        if(count % 31 == 0):\n",
    "            print('downloaded ' + str(count) + ' metadata files so far!')\n",
    "        count += 1\n",
    "    print(\"Successefully downloaded \" + str(len(IDs)) + \" metadatas!\")\n",
    "\n",
    "\n",
    "ISIS_ID_file = open('data/ISIS/IDList.txt', 'r')\n",
    "posIDs = getIDsFromFile(ISIS_ID_file)\n",
    "downloadMetaFiles(posIDs, \"data/ISIS/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the metadata readily available in files, and I/O should take a much faster time to retreive the data rather than downloading them again if need be. We define the methods below to extract the metadata back from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def getMetaFromFile(fileDir):\n",
    "    file = open(fileDir, 'r')\n",
    "    try:\n",
    "        jObject = json.load(file)\n",
    "        return jObject\n",
    "    except ValueError:\n",
    "        print(\"Error reading JSON from \" + fileDir)\n",
    "    \n",
    "def readMetaTextInDirectory(directory):\n",
    "    return [getMetaFromFile(directory + os.sep + f) \n",
    "            for f in os.listdir(directory)\n",
    "            if  f.endswith('.txt')]\n",
    "\n",
    "posMetadata = readMetaTextInDirectory('../data/ISIS/metadata')\n",
    "len(posMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of metadata will be the actual data points which we will process, develop our model with, and test against!\n",
    "We need to get it in VW format first. To see what that looks like, skim through the [input format page](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format). In a nutshell, each data point (metadata entry) must be on its own line. Each line must begin with a +1 or -1 to signify if it's a positive or negative example (ISIS or not ISIS, respectively, in this case), followed by a pipe \"|\" that separated each namespace and its features. The format for namespace and features is: \"namespac1e: feature1 |namespace2: feature2 |namespeace3: feature3...\\n\". \"|\" and \":\" are reserved for VW, so we must distinguish their occurance in the metadata from their occrance in the VW format. We replace \"|\" with \"PIPE\" and \":\" with \"COLON\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def proccessText(text):\n",
    "    newText = text.replace(':', 'COLON').replace('|', 'PIPE').replace('\\n', ' ').replace('\\r', ' ').replace(\"@\", ' ')\n",
    "    newText = newText.replace('<br>', ' ').replace(\"<\\br>\", ' ').replace(\"<span>\", \" \").replace(\"</span>\", \" \")\n",
    "    newText = re.sub('<[^>]*>', '', newText)\n",
    "    newText = newText.replace('الدولة الاسلامية', ' ')\n",
    "    newText = newText.replace('سكر', ' ')\n",
    "    newText = newText.replace(\"gmail.com\", \"\").replace(\"hotmail.com\", \"\").replace(\"yahoo.com\", \"\")\n",
    "    n2 = []\n",
    "    for word in newText.split():\n",
    "        if len(word) < 50:\n",
    "            n2.append(word)\n",
    "    return ' '.join(n2[:500])\n",
    "\n",
    "def metadataToVWline(metadata: dict, positive: bool, weight: int=1):\n",
    "    ignored_keys = ['mediatype', 'sound', 'color', 'curation']\n",
    "    data = ''\n",
    "    if metadata is None:\n",
    "        print('Found null metadata. Skipping.')\n",
    "    else:\n",
    "        for key in metadata:\n",
    "            if key in ignored_keys:\n",
    "                continue\n",
    "            else:\n",
    "                if(type(metadata[key]) == list):\n",
    "                    string = ' '.join(metadata[key])\n",
    "                    data += key + ' ' + proccessText(string) + \" |\"\n",
    "                else:\n",
    "                    data += key + ' ' + proccessText(metadata[key]) + \" |\"\n",
    "        #remove last trailing pipe and add new line\n",
    "        data = data.rstrip('|')\n",
    "        data += '\\n'\n",
    "\n",
    "        if(positive):\n",
    "            data = \"+1 \" + str(weight) + \" |\" + data\n",
    "        else:\n",
    "            data = \"-1 \" + str(weight) + \" |\" + data\n",
    "        \n",
    "    return data\n",
    "\n",
    "#     ID = metadata.get('identifier', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     uploader = metadata.get('uploader', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     title = metadata.get('title', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     subject = metadata.get('subject', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     description = metadata.get('description', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     publishDate = metadata.get('publishdate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     addedDate = metadata.get('addeddate', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     collection = metadata.get('collection', 'NONE').replace(':', 'COLON').replace('|', 'PIPE')\n",
    "#     ID = metadata['identifier']\n",
    "#     uploader = metadata['uploader']\n",
    "#     title = metadata['title']\n",
    "#     subject = metadata['subject']\n",
    "#     description = metadata['description']\n",
    "#     publishDate = metadata['publicdate']\n",
    "#     addedDate = metadata['addeddate']\n",
    "#     collection = metadata['collection']\n",
    "#     print(metadata)\n",
    "    \n",
    "#     data = \"identifier: \".join(ID) + \" | uploader: \".join(uploader) + \" | title: \".join(title) + \" | subject: \"\n",
    "#     data = data + ''.join(subject) + \" | description\".join(description) + \" | publishDate: \".join(publishDate)\n",
    "#     data = data + \" | addedDate: \".join(addedDate) + \" | collection: \".join(collection) + \"\\n\"\n",
    "\n",
    "\n",
    "def writeVWlinesToFile(vwLines, file):\n",
    "    print(\"opening Vwneg.txt file to write metadata\")\n",
    "    print(\"Writing lines to file...\")\n",
    "    for line in vwLines:\n",
    "        file.write(line)\n",
    "    print(\"Closing file...\")\n",
    "    file.close()\n",
    "    print(\"File closed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening Vwneg.txt file to write metadata\n",
      "Writing lines to file...\n",
      "Closing file...\n",
      "File closed!\n"
     ]
    }
   ],
   "source": [
    "posVwLines = []\n",
    "for meta in posMetadata:\n",
    "    posVwLines.append(metadataToVWline(meta, True))\n",
    "    \n",
    "ISIS_Metadata_vw_file = open('VWpos.txt', 'w')\n",
    "writeVWlinesToFile(posVwLines, ISIS_Metadata_vw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* ISIS content that violated terms of service). The accurasy of our assumption was recorded as \n",
    "\n",
    "\n",
    "$AccViol = \\frac{v(s)}{l(s)}$\n",
    "\n",
    "where $v(s)$ is the number of items violating the terms of service in the sample $s$ and $l(s)$ is the number of items in $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.shuffle(metadata)\n",
    "# AccViol = 27/30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main points:\n",
    "- get negative data\n",
    "- run it all through VW\n",
    "- get a proccess to get new data (IDs + metadata)\n",
    "\n",
    "##### Arabic-non-ISIS Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another collectino of ~5,000 items that included general arabic words was collected. This collection would represent items that included arabic text but were not violating of the terms of service. A general keyword was needed for searching content not related to ISIS. سكر (sugar) was chosen. This keyword surprisingly collected a lot of quran recitations, which is good since that means the data will force the model to not discremenate based on religious text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mkdir ../data/Arabic-non-ISIS\n",
    "! mkdir ../data/Arabic-non-ISIS/metadata\n",
    "! echo \"Number of matching searches found : \"\n",
    "! ia search \"سكر\" -i -n \n",
    "! echo \"Donwloading item identifiers...\"\n",
    "! ia search \"سكر\" -i > ../data/Arabic-non-ISIS/existingIDs.txt\n",
    "! echo \"Number of item identifiers downloaded:\"\n",
    "! wc -l ../data/Arabic-non-ISIS/metadata/existingIDs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SINCE THERE COULD BE A LOT OF METADATA TO DOWNLOAD, PLEASE FEEL FREE TO STOP THIS KERNEL PROCCESS MANUALLY WHEN YOU THINK YOU HAVE ENOUGH DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Arabic_non_ISIS_ID_file = open('../data/Arabic-non-ISIS/existingIDs.txt', 'r')\n",
    "negIDs = getIDsFromFile(Arabic_non_ISIS_ID_file)\n",
    "downloadMetaFiles(posIDs, \"../data/Arabic-non-ISIS/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the metadata that we downloaded into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4265"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negMetadata = readMetaTextInDirectory('../data/Arabic-non-ISIS/metadata')\n",
    "len(negMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the items in this list into lines readable by vw, and then we write these lines to a file for backup and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negVwLines = []\n",
    "for meta in negMetadata:\n",
    "    negVwLines.append(metadataToVWline(meta, False))\n",
    "    \n",
    "# Arabic_metadata_VW_file = open('data/Arabic-non-ISIS/VWneg.txt', 'w')\n",
    "# writeVWlinesToFile(negVwLines, Arabic_metadata_VW_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample (~40 items) of this collection was inspected to determine the accuracy of the assumption (how many items in the sample are *actually* general arabic content that do not violate the terms of service). The accuracy of our assumption was recorded as $AccAra$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of negative and positive examples are combined and shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5351"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allVWLines = posVwLines + negVwLines\n",
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(allVWLines)\n",
    "len(allVWLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split in half the shuffled vw lines into training and testing data. We write both to their appropriate file. *sentiment.tr* will be our training data, and *sentiment.te* will be our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1400 data/sentiment.tr\n",
      "    3951 data/sentiment.te\n",
      "    5351 total\n"
     ]
    }
   ],
   "source": [
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            h.write(ex)\n",
    "writeToVWFile('sentiment.tr', allVWLines[:1400])\n",
    "writeToVWFile('sentiment.te', allVWLines[1400:])\n",
    "!wc -l sentiment.tr sentiment.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the model against our training data and cross our fingers that it learns something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.800000 0.800000            2            1.2   1.0000  -1.0000      527\n",
      "0.727273 0.666667            5            2.8   1.0000  -1.0000      526\n",
      "0.608696 0.500000           11            5.8   1.0000   1.0000       55\n",
      "0.304348 0.000000           25           11.5  -1.0000  -1.0000      523\n",
      "0.163043 0.021739           53           23.0  -1.0000  -1.0000      527\n",
      "0.085561 0.010526          100           46.8   1.0000   1.0000       43\n",
      "0.053476 0.021390          221           93.5  -1.0000  -1.0000      528\n",
      "0.026631 0.000000          448          187.8   1.0000   1.0000       30\n",
      "0.019308 0.011984          899          375.5  -1.0000  -1.0000      544\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1400\n",
      "passes used = 1\n",
      "weighted example sum = 572.000000\n",
      "weighted label sum = 20.000000\n",
      "average loss = 0.016608\n",
      "best constant = 0.034965\n",
      "best constant's loss = 0.998778\n",
      "total feature number = 612772\n"
     ]
    }
   ],
   "source": [
    "!vw --binary sentiment.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see what it looks like, it is time to train vw over the data sufficiently (with many passes) and produce a model file that will be used to make predictions against the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.800000 0.800000            2            1.2   1.0000  -1.0000      527\n",
      "0.727273 0.666667            5            2.8   1.0000  -1.0000      526\n",
      "0.590909 0.454545           10            5.5   1.0000   1.0000       55\n",
      "0.318182 0.045455           23           11.0  -1.0000  -1.0000      523\n",
      "0.170455 0.022727           52           22.0  -1.0000  -1.0000      525\n",
      "0.085227 0.000000           98           44.0  -1.0000  -1.0000      530\n",
      "0.053977 0.022727          211           88.0  -1.0000  -1.0000      583\n",
      "0.026989 0.000000          416          176.0   1.0000   1.0000       23\n",
      "0.021307 0.015625          847          352.0  -1.0000   1.0000      557\n",
      "0.013333 0.013333         1703          704.0  -1.0000  -1.0000      528 h\n",
      "0.008130 0.003175         3430         1408.0  -1.0000  -1.0000      531 h\n",
      "0.006574 0.004983         6857         2816.0  -1.0000  -1.0000      530 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1260\n",
      "passes used = 6\n",
      "weighted example sum = 3096.000000\n",
      "weighted label sum = 120.000000\n",
      "average loss = 0.004464 h\n",
      "best constant = 0.038760\n",
      "best constant's loss = 0.998498\n",
      "total feature number = 3307890\n"
     ]
    }
   ],
   "source": [
    "! vw --binary sentiment.tr --passes 20 -c -k -f sentiment.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a model in data/sentiment.model! Notice that the **average loss** is incredibly low when the model is finally created, which means that the extra passes improved the model, and the last time it ran through the model it could predict (persumably) all of the items correctly.\n",
    "Since we have a model, it is now time to run it against the testing data we left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = data/sentiment.te.pred\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.te\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            4            1.0  -1.0000  -1.0000      526\n",
      "0.000000 0.000000            8            2.0  -1.0000  -1.0000      542\n",
      "0.000000 0.000000           13            4.0   1.0000   1.0000       23\n",
      "0.000000 0.000000           23            8.0  -1.0000  -1.0000      535\n",
      "0.000000 0.000000           36           16.5   1.0000   1.0000       21\n",
      "0.000000 0.000000           78           33.0  -1.0000  -1.0000      532\n",
      "0.000000 0.000000          159           66.8   1.0000   1.0000       32\n",
      "0.001873 0.003745          327          133.5   1.0000   1.0000       22\n",
      "0.005618 0.009363          636          267.0  -1.0000  -1.0000      535\n",
      "0.003277 0.000936         1314          534.0  -1.0000  -1.0000      542\n",
      "0.007257 0.011236         2646         1068.0  -1.0000  -1.0000      527\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 3951\n",
      "passes used = 1\n",
      "weighted example sum = 1580.250000\n",
      "weighted label sum = -0.250000\n",
      "average loss = 0.008543\n",
      "best constant = -0.000158\n",
      "best constant's loss = 1.000000\n",
      "total feature number = 1749771\n"
     ]
    }
   ],
   "source": [
    "! vw --binary -t -i sentiment.model -p sentiment.te.pred sentiment.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average loss is only 0.011, which means that the model is 98.9% accurate! Since this is a little suspecious, let us look at what the most predictive features were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!vw -i sentiment.model -t --invert_hash sentiment.model.readable sentiment.tr --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As copied from GettingTheMost vwnlp notebook: this says \"start from that pre-trained model; go into test mode (so that you don't adjust any of the weights of the model); store the resulting readable model (--invert_hash) into the specified file; and read from data/sentiment.tr (you have to re-read from the same training data).\n",
    "We can now look at data/sentiment.model.readable to see what's going on.\"\n",
    "\n",
    "The next step after making this readable copy of the model, we need to inspect it. To do that, we will sort the data by the feature weight (details in the refereed book). We look at the 30 most predictive features by -n30 at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description^وكان:81050:9.59125e-05\r\n",
      "description^يا:235434:9.51413e-06\r\n",
      "description^لوحده:121288:9.05609e-05\r\n",
      "segments^كتابات:121288:9.05609e-05\r\n",
      "collection^opensource_movies:102092:0.163087\r\n",
      "uploader^mail.ru:184078:0.115139\r\n",
      "uploader^oma_222:64648:0.103694\r\n",
      "scanner^Archive:202608:0.103688\r\n",
      "scanner^HTML5:130839:0.103688\r\n",
      "scanner^Internet:229369:0.103688\r\n",
      "scanner^Uploader:37599:0.103688\r\n",
      "description^ولاية:255536:0.0875161\r\n",
      "subject^ولاية:246401:0.0829801\r\n",
      "scanner^1.6.3:27600:0.082559\r\n",
      "title^في:64074:0.0740446\r\n",
      "uploader^gmail.com:184177:0.0674716\r\n",
      "collection^loggedin:13007:0.0640019\r\n",
      "subject^العراق:205019:0.06207\r\n",
      "subject^في:202856:0.0564585\r\n",
      "title^الدولة:104376:0.0534749\r\n",
      "uploader^fofo.bobo.82:169643:0.0503102\r\n",
      "description^size=\"5\"><b>-4</b></font></u></font><br:51670:0.0457658\r\n",
      "subject^الخير:51670:0.0457658\r\n",
      "scanner^1.6.0:9932:0.0428825\r\n",
      "description^باقية:220112:0.0428049\r\n",
      "title^من:174608:0.0418559\r\n",
      "creator^المهاجر:132187:0.0417945\r\n",
      "collection^iraq_middleeast:218420:0.0415523\r\n",
      "collection^iraq_war:117857:0.0415523\r\n",
      "collection^newsandpublicaffairs:164064:0.0415523\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head -n30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Scanner and Creator\n",
    "Since we are not particularly intereted by what scanner was used to scan the items when they were added to the Interner Archive, we predict that the scanner namespace is not a necessarily important one, so we would like to remove it. Furthrmore, we don't want the model to be biased against items in one collection or another, so we remove that namespace as well. We remove these namespaces by ignoring them when we are proccessing the metadata. So we redefine that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metadataToVWline(metadata: dict, positive: bool):\n",
    "    ignored_keys = ['mediatype', 'sound', 'color', 'curation', 'creator', 'scanner', 'collection']\n",
    "    data = ''\n",
    "    if metadata is None:\n",
    "        print('Found null metadata. Skipping.')\n",
    "    else:\n",
    "        for key in metadata:\n",
    "            if key in ignored_keys:\n",
    "                continue\n",
    "            else:\n",
    "                if(type(metadata[key]) == list):\n",
    "                    string = ' '.join(metadata[key])\n",
    "                    data += key + ' ' + proccessText(string) + \" |\"\n",
    "                else:\n",
    "                    data += key + ' ' + proccessText(metadata[key]) + \" |\"\n",
    "        #remove last trailing pipe and add new line\n",
    "        data = data.rstrip('|')\n",
    "        data += '\\n'\n",
    "\n",
    "        if(positive):\n",
    "            data = \"+1 |\" + data\n",
    "        else:\n",
    "            data = \"-1 |\" + data\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-proccess the metadata and run it through vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/sentiment.model\n",
      "Num weight bits = 30\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000     1025\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000     1034\n",
      "0.250000 0.000000            4            4.0  -1.0000  -1.0000     1057\n",
      "0.375000 0.500000            8            8.0   1.0000   1.0000       30\n",
      "0.187500 0.000000           16           16.0  -1.0000  -1.0000     1044\n",
      "0.125000 0.062500           32           32.0  -1.0000  -1.0000     1014\n",
      "0.078125 0.031250           64           64.0   1.0000   1.0000       30\n",
      "0.039062 0.000000          128          128.0  -1.0000  -1.0000     1022\n",
      "0.019531 0.000000          256          256.0   1.0000   1.0000       21\n",
      "0.011719 0.003906          512          512.0  -1.0000  -1.0000     1056\n",
      "0.009766 0.007812         1024         1024.0  -1.0000   1.0000       42\n",
      "0.008301 0.006836         2048         2048.0  -1.0000  -1.0000     1039\n",
      "0.017621 0.017621         4096         4096.0   1.0000   1.0000       24 h\n",
      "0.014317 0.011013         8192         8192.0  -1.0000  -1.0000     1032 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 2408\n",
      "passes used = 6\n",
      "weighted example sum = 14448.000000\n",
      "weighted label sum = -8292.000000\n",
      "average loss = 0.007491 h\n",
      "best constant = -0.573920\n",
      "best constant's loss = 0.670616\n",
      "total feature number = 12281634\n"
     ]
    }
   ],
   "source": [
    "posVwLines = []\n",
    "for meta in posMetadata:\n",
    "    posVwLines.append(metadataToVWline(meta, True))\n",
    "negVwLines = []\n",
    "for meta in negMetadata:\n",
    "    negVwLines.append(metadataToVWline(meta, False))\n",
    "allVWLines = posVwLines + negVwLines\n",
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(allVWLines)\n",
    "mid = int(len(allVWLines)/2)\n",
    "\n",
    "writeToVWFile('sentiment.tr', allVWLines[:mid])\n",
    "writeToVWFile('sentiment.te', allVWLines[mid:])\n",
    "! vw --binary sentiment.tr --passes 20 -c -k -f sentiment.model  -b 30 --ngram 2\n",
    "! vw -i sentiment.model -t --invert_hash sentiment.model.readable sentiment.tr --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploader^mohmal.com:822019353:9.95211e-05\r\n",
      "description^لينين:96284365:9.9512e-05\r\n",
      "description^في^دعوة:770908253:9.75299e-05\r\n",
      "description^في^تقرير:433196936:9.75299e-05\r\n",
      "description^ابن^كمال:983901042:9.75299e-05\r\n",
      "description^على^الفرق:1006822134:9.75299e-05\r\n",
      "description^عند^اليهود:20357177:9.75299e-05\r\n",
      "description^أثر^الانحراف:590395177:9.75299e-05\r\n",
      "description^باشا:209253963:9.75299e-05\r\n",
      "description^إلهي:664814250:9.75299e-05\r\n",
      "description^ظهير:978343008:9.75299e-05\r\n",
      "description^كمال^باشا:560267548:9.75299e-05\r\n",
      "description^إلهي^ظهير:713772642:9.75299e-05\r\n",
      "description^باشا^وآراؤه:437705652:9.75299e-05\r\n",
      "description^ظهير^وجهوده:721565426:9.75299e-05\r\n",
      "description^إحسان:556642123:9.75299e-05\r\n",
      "description^والرد^على:281982678:9.75299e-05\r\n",
      "description^نقدية^على:488202728:9.75299e-05\r\n",
      "description^إحسان^إلهي:261321673:9.75299e-05\r\n",
      "description^الشيخ^إحسان:389946519:9.75299e-05\r\n",
      "description^تقرير^العقيدة:280135625:9.75299e-05\r\n",
      "description^الفكر^الصهيوني:1008258838:9.75299e-05\r\n",
      "description^الفرق^المخالفة:390275590:9.75299e-05\r\n",
      "description^اليهود^على:829766304:9.75299e-05\r\n",
      "description^العقدي^والفكري:6890807:9.75299e-05\r\n",
      "description^الدعوة^الاسلامية:890106767:9.75299e-05\r\n",
      "description^والفكري:770171201:9.75299e-05\r\n",
      "description^والفكري^عند:514667950:9.75299e-05\r\n",
      "description^العقيدة^والرد:466414144:9.75299e-05\r\n",
      "description^المخالفة:515321676:9.75299e-05\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat sentiment.model.readable  | tail -n+13 | sort -t: -k3nr | head -n30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Placeholder code\")\n",
    "ia search 'الصداقة' -i -n\n",
    "ia search 'سكر' -i -n\n",
    "\n",
    "AccAra = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
